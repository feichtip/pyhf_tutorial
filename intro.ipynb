{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from platform import python_version\n",
        "from pprint import pprint\n",
        "\n",
        "import cabinetry\n",
        "import hist\n",
        "import iminuit\n",
        "import jax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mplhep\n",
        "import numpy as np\n",
        "import pyhf\n",
        "from hist import Hist\n",
        "from tabulate import tabulate\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = [8.0, 6.0]\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['axes.labelsize'] = 'large'\n",
        "\n",
        "np.random.seed(1010)\n",
        "\n",
        "\n",
        "print(f'{iminuit.__version__ = }')  # 2.18.0\n",
        "print(f'{pyhf.__version__ = }')  # 0.7.0\n",
        "print(f'{cabinetry.__version__ = }')  # 0.5.1\n",
        "print(f'{hist.__version__ = }')  # 2.6.1\n",
        "print(f'{jax.__version__ = }')  # 0.3.6\n",
        "print(f'{np.__version__ = }')  # 1.21.5\n",
        "print(f'{python_version() = }')  # 3.8.10\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pyhf\n",
        "  - documentation: https://pyhf.readthedocs.io/en/latest/\n",
        "  - tutorials: https://pyhf.github.io/pyhf-tutorial/introduction.html\n",
        "  - build complex models out of easy to handle building blocks\n",
        "  - fitting + limit setting\n",
        "  - easy way of storing the model\n",
        "  - only for binned fits\n",
        "- cabinetry\n",
        "  - documentation: https://cabinetry.readthedocs.io/en/latest/\n",
        "  - conference paper: https://www.epj-conferences.org/articles/epjconf/pdf/2021/05/epjconf_chep2021_03067.pdf\n",
        "  - provides many high level convenience functions on top of pyhf\n",
        "  - has its own configuration file that can be used to build a pyhf model (I don't do this, but could be useful)\n",
        "  - provides less flexibility than when just using pyhf\n",
        "  - still has some bugs, use with caution and cross-check results\n",
        "- PyHEP / SciPy conference talks\n",
        "  - 2022: https://indico.cern.ch/event/1150631\n",
        "  - iminuit, pyhf, cabinetry\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# first, we create some toy MC and data: a gaussian signal on top of exponential background\n",
        "\n",
        "bkg = np.random.exponential(scale=8, size=5_000)  # 5000 MC events for signal\n",
        "sig = np.random.normal(loc=10, size=5_000)  # 5000 MC events for background\n",
        "\n",
        "# fill a histogram with the hist library\n",
        "ax = hist.axis.Regular(15, 0, 18, name='M')\n",
        "h_bkg = Hist(ax, storage=hist.storage.Weight()).fill(bkg)\n",
        "\n",
        "# weight signal with 0.1, only 500 signal events are expected in our toy experiment\n",
        "# (but we use the larger MC sample for smaller uncertainties)\n",
        "h_sig = Hist(ax, storage=hist.storage.Weight()).fill(sig, weight=0.1)\n",
        "\n",
        "# some of the signal bins have 0 entries, this causes issues with some cabinetry functions\n",
        "# we can set the values and variance of those bins to a small number\n",
        "# pyhf can handle 0 entry bins since version 0.7.0, but the observed data should also be 0 in those bins\n",
        "h_sig.variances()[h_sig.values() == 0] = 1E-6\n",
        "h_sig.values()[h_sig.values() == 0] = 1E-4\n",
        "\n",
        "# show our toy MC, note the statistical uncertainty for singal and background\n",
        "mplhep.histplot([h_bkg, h_sig], label=['background', 'signal'], stack=True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# now we generate our toy data by sampling from a poission\n",
        "\n",
        "data = np.concatenate([np.random.exponential(scale=8, size=np.random.poisson(5_000)),\n",
        "                       np.random.normal(loc=10, size=np.random.poisson(500))])\n",
        "h_data = Hist(ax, storage=hist.storage.Int64()).fill(data)\n",
        "\n",
        "mplhep.histplot([h_bkg, h_sig], label=['background', 'signal'], stack=True)\n",
        "h_data.plot(histtype='errorbar', color='k')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# this function creates a pyhf model as a python dictionary from our MC and data histograms\n",
        "# it can also be saved as a json file and directly read by pyhf\n",
        "\n",
        "\n",
        "def create_model(h_sig, h_bkg, h_data, bkg_norm='normsys', save=False):\n",
        "    \"\"\"\n",
        "    basic structure of the dictionary:\n",
        "    {'channels': [{'name': 'channel_name',\n",
        "                   'samples': [{'data': [...],\n",
        "                                'modifiers': [{'data': None,\n",
        "                                               'name': 'mu',\n",
        "                                               'type': 'normfactor'},\n",
        "                                              {'data': [...],\n",
        "                                               'name': 'sig_stat_error',\n",
        "                                               'type': 'staterror'}],\n",
        "                                'name': 'signal'},\n",
        "                               {'data': [...],\n",
        "                                'modifiers': [{'data': {'hi': X.YZ, 'lo': X.YZ},\n",
        "                                               'name': 'bkg_norm',\n",
        "                                               'type': 'normsys'},\n",
        "                                              {'data': [...],\n",
        "                                               'name': 'bkg_stat_error',\n",
        "                                               'type': 'staterror'}],\n",
        "                                'name': 'background'}]}],\n",
        "     'measurements': [{'config': {'parameters': [], 'poi': 'mu'},\n",
        "                       'name': 'Measurement'}],\n",
        "     'observations': [{'data': [...],\n",
        "                       'name': 'channel_name'}],\n",
        "     'version': '1.0.0'}\n",
        "    \"\"\"\n",
        "\n",
        "    model_dict = {'measurements': [],\n",
        "                  'observations': [],\n",
        "                  'channels': [],\n",
        "                  'version': '1.0.0'}\n",
        "\n",
        "    model_dict['measurements'].append(\n",
        "        {\"name\": \"Measurement\", \"config\": {\"poi\": \"mu\", \"parameters\": []}}\n",
        "    )\n",
        "\n",
        "    model_dict['observations'].append(\n",
        "        {\"name\": 'channel_1',\n",
        "         \"data\": list(h_data.values().astype(float))}\n",
        "    )\n",
        "\n",
        "    model_dict['channels'].append({\n",
        "        'name': 'channel_1',\n",
        "        'samples': []\n",
        "    })\n",
        "\n",
        "    model_dict['channels'][0]['samples'].append({\n",
        "        'name': 'signal',\n",
        "        'data': list(h_sig.values()),\n",
        "        'modifiers': [\n",
        "            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None},\n",
        "            {\"name\": \"sig_stat_error\",\n",
        "             \"type\": \"staterror\",\n",
        "             \"data\": list(np.sqrt(h_sig.variances()))}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    bkg_modifiers = []\n",
        "    if bkg_norm == 'normsys':  # constrained background normalisation\n",
        "        bkg_modifiers.append({\"name\": \"bkg_norm\", \"type\": \"normsys\", \"data\": {\"hi\": 1.02, \"lo\": 0.98}})\n",
        "    elif bkg_norm == 'normfactor':  # free floating background\n",
        "        bkg_modifiers.append({\"name\": \"bkg_norm\", \"type\": \"normfactor\", \"data\": None})\n",
        "\n",
        "    bkg_modifiers.append({\"name\": \"bkg_stat_error\",\n",
        "                          \"type\": \"staterror\",\n",
        "                          \"data\": list(np.sqrt(h_bkg.variances()))})\n",
        "\n",
        "    model_dict['channels'][0]['samples'].append({\n",
        "        'name': 'background',\n",
        "        'data': list(h_bkg.values()),\n",
        "        'modifiers': bkg_modifiers\n",
        "    })\n",
        "\n",
        "    pyhf.schema.validate(model_dict, 'workspace.json')\n",
        "\n",
        "    if save:\n",
        "        model_string = json.dumps(model_dict, sort_keys=True, indent=4)\n",
        "        with open('workspace.json', 'w') as outfile:\n",
        "            outfile.write(model_string)\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "\n",
        "# in 'measurements' -> 'config' -> 'parameters' we could set inital values of the parameters,\n",
        "# which would be our SM expectation, e.g. {\"name\": \"mu\", \"inits\": [2.0]}\n",
        "model_dict = create_model(h_sig, h_bkg, h_data, bkg_norm='normsys', save=True)\n",
        "\n",
        "pprint(model_dict)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyhf also has a command line interface\n",
        "# ! pyhf\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ! pyhf inspect workspace.json\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## pyhf backends/optimizers\n",
        "\n",
        "What is best? Very much depens on your problem, try different combinations.\n",
        "\n",
        "---\n",
        "\n",
        "- backends\n",
        "  - numpy\n",
        "  - jax\n",
        "  - pytorch\n",
        "  - tensorflow\n",
        "\n",
        "\n",
        "I found jax to be faster than numpy, but somethimes the optimisation fails with jax but works with numpy.\n",
        "\n",
        "---\n",
        "\n",
        "- optimizers\n",
        "  - (i)minuit\n",
        "  - scipy.optimize\n",
        "\n",
        "\n",
        "When you are perfroming a fit and you need uncertainties, use minuit (cabinetry does this by default).\n",
        "When you don't need uncertainties (toys/limit setting) scipy can be faster and easier to handle.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set pyhf backend\n",
        "pyhf.set_backend('jax', 'minuit')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get model and data object with cabinetry\n",
        "# model_dict = cabinetry.workspace.load(\"workspace.json\")  # from json file\n",
        "model, data = cabinetry.model_utils.model_and_data(model_dict)  # use python dict directly\n",
        "\n",
        "model\n",
        "data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# simple fit with cabinetry api, run MINOS for parameter of interest mu\n",
        "fit_results = cabinetry.fit.fit(model, data, minos=['mu'])\n",
        "minos_unc = fit_results.minos_uncertainty['mu']\n",
        "print('MINOS:', minos_unc)  # MINOS asymmetric uncertainties\n",
        "print('2NLL:', fit_results.best_twice_nll)  # value of -2LL at the fitted parameters\n",
        "print(tabulate(np.array([fit_results[i] for i in [2, 0, 1]]).T))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model.config is very useful to get information about the model\n",
        "\n",
        "model.config.npars\n",
        "model.config.par_order\n",
        "model.config.par_names\n",
        "# model.config.par_slice('mu')\n",
        "# model.config.par_slice('bkg_stat_error')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# same fit as before, but with pyhf api\n",
        "# this has the advantage that we have access to the minuit object\n",
        "par_estimates, results = pyhf.infer.mle.fit(data, model, return_result_obj=True, return_uncertainties=True)\n",
        "print(tabulate([(par, par_estimates[model.config.par_map[par]['slice']]) for par in model.config.par_order]))\n",
        "\n",
        "# results.minuit.fmin\n",
        "# results.minuit.params\n",
        "# results.minuit.covariance\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# performs a profile likelihood scan with cabinetry (should be the same as MINOS profile)\n",
        "scan_results = cabinetry.fit.scan(model, data, \"mu\")\n",
        "cabinetry.visualize.scan(scan_results, save_figure=False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we can now compare the fit with 3 different implementations on the background normalisation\n",
        "# - None: fixed background\n",
        "# - normsys: constrained background normalisation\n",
        "# - normfactor: free floating background normalisation\n",
        "\n",
        "profiles = {}\n",
        "poi_estimates = []\n",
        "for i, bkg_norm in enumerate([None, 'normsys', 'normfactor']):\n",
        "    model_dict = create_model(h_sig, h_bkg, h_data, bkg_norm=bkg_norm)\n",
        "    model, data = cabinetry.model_utils.model_and_data(model_dict)\n",
        "    par_estimates, results = pyhf.infer.mle.fit(data, model, return_result_obj=True, return_uncertainties=True)\n",
        "    profiles[bkg_norm] = results.minuit.draw_mnprofile(model.config.poi_name, band=False, text=False)\n",
        "    poi_estimates.append(par_estimates[model.config.poi_index])\n",
        "plt.hlines(1, *plt.xlim(), color='gray')\n",
        "plt.vlines(1, *plt.ylim(), color='gray', ls=':')\n",
        "plt.ylabel(r'-2$\\Delta$LL')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for i, poi_estimate in enumerate(poi_estimates):\n",
        "    plt.errorbar(poi_estimate[0].item(), -i * 0.1, xerr=poi_estimate[1].item(), marker='o')\n",
        "plt.vlines(1, -1, 1, color='k', ls='--', alpha=0.5)\n",
        "plt.yticks(ticks=[0, -0.1, -0.2], labels=['fixed bkg', 'constrained bkg', 'floating bkg'])\n",
        "plt.ylim(-0.3, 0.1)\n",
        "plt.xlabel(r'$\\mu$')\n",
        "plt.show()\n",
        "\n",
        "# we end up with slightly different MLE, but more interesting are the uncertainties of our POI\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we can get a nice visual comparison of the uncertainty when we shift the parabulas to 0\n",
        "\n",
        "for (key, value), poi_estimate, label in zip(profiles.items(),\n",
        "                                             poi_estimates,\n",
        "                                             ['fixed bkg', 'constrained bkg', 'floating bkg']):\n",
        "    plt.plot(value[0] - poi_estimate[0].item(), value[1], label=label)\n",
        "plt.xlabel(r'$\\mu - \\hat{\\mu}$')\n",
        "plt.ylabel(r'-2$\\Delta$LL')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# with the minuit object we can also draw 2D MINOS contours, here for the 68% confidence region in mu vs bkg_norm\n",
        "# (we use the model with the floating background)\n",
        "results.minuit.draw_mncontour('mu', 'bkg_norm')\n",
        "plt.plot(1, 1, marker='+', ms=12)\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# signal normalisation and background normalisation are anti-correlated, as we can also see from the covariance (correlation) matrix\n",
        "results.minuit.covariance\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}