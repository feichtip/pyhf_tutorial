{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cabinetry\n",
        "import hist\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pyhf\n",
        "import utils\n",
        "from hist import Hist\n",
        "from IPython.display import Image\n",
        "from pyhf.contrib.viz import brazil\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = [8.0, 6.0]\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['axes.labelsize'] = 'large'\n",
        "\n",
        "np.random.seed(1010)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# get model from before, with constrained background (normsys)\n",
        "model_dict = cabinetry.workspace.load(\"workspace.json\")  # from json file\n",
        "model, data = cabinetry.model_utils.model_and_data(model_dict)\n",
        "\n",
        "# fit and show pre/post-fit distributions\n",
        "_ = utils.fit_model(model, data)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a function to sample data from our underlying model\n",
        "\n",
        "\n",
        "def sample_data(bkg_scale=1, sig_scale=1):\n",
        "    axis = hist.axis.Regular(15, 0, 18)\n",
        "    data = np.concatenate([np.random.exponential(scale=8, size=(np.random.poisson(bkg_scale * 5_000))),\n",
        "                           np.random.normal(loc=10, size=(np.random.poisson(sig_scale * 500)))])\n",
        "\n",
        "    h = Hist(axis, storage=hist.storage.Int64()).fill(data)\n",
        "    return h\n",
        "\n",
        "\n",
        "sample_data(1, 1)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data in pyhf\n",
        "the data for our model always contains the bin entries ('maindata') and data associated with nuisance parameters ('auxdata')\n",
        "\n",
        "this can be a little bit confusing at first, but since we work in a frequentist framework it is the correct way to set up the likelihood\n",
        "\n",
        "in principle this can be additional information coming from a control channel, but for most appliciations of pyhf this\n",
        "should be left to the suggested initialisation paramters (information about the size of the systematics which manifests\n",
        "itself in the constraint terms is already included in the model from the specification)\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Image(filename='fit_model.png')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename='modifiers.png')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data  # passed to the fit, always maindata + auxdata\n",
        "model.config.nmaindata  # number of maindat\n",
        "model.config.nauxdata  # number of auxdata\n",
        "model.config.auxdata  # only the auxdata for the specific model\n",
        "model.config.auxdata_order  # order of the modifiers corresponding to the auxdata\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to fit our model to the sampled data we have to merge it first with the auxdata\n",
        "toy_data = list(sample_data(bkg_scale=1.0, sig_scale=1.0).values()) + model.config.auxdata\n",
        "\n",
        "# we can also fit our model to the expected data\n",
        "# toy_data = model.expected_data(model.config.suggested_init())\n",
        "\n",
        "fit_results = utils.fit_model(model, toy_data)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- we have a 2% uncertainty on the background normalisation in our model\n",
        "- the fitted parameter ('bkg_norm') in this case corresponds to the 'pull' away from the expected normalisation (our nominal MC template)\n",
        "- this is, how many standard deviations (in our case 1 sigma = 2%) does the fitted parameter deviate?\n",
        "- pull plot with cabinetry is currently bugged, only correct for gaussian constraints centered at 0\n",
        "---\n",
        "how does the bkg level affect the fitted parameters?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cabinetry.visualize.pulls(fit_results, exclude=[par for par in model.config.par_names if 'stat' in par] + [model.config.poi_name], save_figure=False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a more systematic way to study this is with a toy study\n",
        "\n",
        "# pyhf.set_backend('numpy')\n",
        "pyhf.set_backend('jax')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# with pyhf we can easily sample data from our model\n",
        "# this will also sample auxdata from the constraint terms\n",
        "# it is important to include this, since we want to sample from our whole model with all systematic variations\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n_toys = 100\n",
        "pars = model.config.suggested_init()\n",
        "# pars[model.config.poi_index] = 1.0\n",
        "toys = model.make_pdf(pyhf.tensorlib.astensor(pars)).sample((n_toys,))\n",
        "toys[0]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bestfits = []\n",
        "uncertainties = []\n",
        "\n",
        "for toy in tqdm(toys):\n",
        "    # toy_data = toy  # if we want to use the expected data from our model\n",
        "\n",
        "    # here we replace the maindata part of our toy with data sampled with our function, so we can change the underlying signal/background scale\n",
        "    toy_data = list(sample_data(bkg_scale=1.0, sig_scale=1.0).values()) + list(toy[model.config.nmaindata:])\n",
        "\n",
        "    fit_results = utils.fit_model(model, toy_data, verbose=False)\n",
        "    if fit_results:\n",
        "        bestfits.append(fit_results.bestfit)\n",
        "        uncertainties.append(fit_results.uncertainty)\n",
        "\n",
        "bestfits = np.array(bestfits)\n",
        "uncertainties = np.array(uncertainties)\n",
        "\n",
        "# "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for par_name in ['mu', 'bkg_norm']:\n",
        "    par_slice = model.config.par_slice(par_name)\n",
        "    pull = (bestfits[:, par_slice] - model.config.suggested_init()[par_slice]) / uncertainties[:, par_slice]\n",
        "    utils.fit_pull(pull, show_bins=20, xlabel=f'pull ({par_name})')\n",
        "    plt.show()\n",
        "    print(f'mean: {pull.mean():.4f}, std: {pull.std():.4f}')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pyhf also provides a nice framework to compute confidence intervals set limits on the POI\n",
        "- lets assume we don't observe a clear signal in our data, what is its significance and the upper limit on the signal strength?\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(12)\n",
        "toy_data = list(sample_data(bkg_scale=1.0, sig_scale=0.05).values()) + model.config.auxdata\n",
        "_ = utils.fit_model(model, toy_data)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we can compute the observed and expected significance (for mu=1) with cabinetry\n",
        "cabinetry.fit.significance(model, toy_data)\n",
        "# or directly with pyhf\n",
        "pyhf.infer.hypotest(0, toy_data, model, test_stat=\"q0\", return_expected_set=True)  # returns p-values\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# do a parameter scan over different poi values to set a upper limit on mu\n",
        "poi_vals = np.linspace(0, 0.5, 21)\n",
        "results = [\n",
        "    pyhf.infer.hypotest(\n",
        "        test_poi, toy_data, model, test_stat=\"qtilde\", return_expected_set=True,\n",
        "    )\n",
        "    for test_poi in poi_vals\n",
        "]\n",
        "\n",
        "# the brazil band in this case shows the expected limit for mu=0 !\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(7, 5)\n",
        "brazil.plot_results(poi_vals, results, ax=ax)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# calculate upper limit with interpolation\n",
        "observed = np.asarray([h[0] for h in results]).ravel()\n",
        "expected = np.asarray([h[1][2] for h in results]).ravel()\n",
        "print(f\"Upper limit (obs): μ = {np.interp(0.05, observed[::-1], poi_vals[::-1]):.4f}\")\n",
        "print(f\"Upper limit (exp): μ = {np.interp(0.05, expected[::-1], poi_vals[::-1]):.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}